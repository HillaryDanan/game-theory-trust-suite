{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Human-AI Trust Laboratory\n",
        "\n",
        "*Build and break trust with AI agents that learn from your behavior!*\n",
        "\n",
        "This implementation explores:\n",
        "- Transparent belief state architectures for AI agents\n",
        "- Adaptive trust calibration based on behavioral monitoring\n",
        "- Multiple AI strategies representing different alignment approaches\n",
        "\n",
        "Based on research in Multi-Objective Reinforcement Learning (MORL) and adaptive trust frameworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install plotly pandas numpy ipywidgets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ü§ñ Human-AI Trust Laboratory { display-mode: \"form\" }\n",
        "#@markdown Build and break trust with AI agents that learn from your behavior!\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "# Trust-based color scheme\n",
        "AI_COLORS = {\n",
        "    'confident': '#00D9FF',      # Bright cyan\n",
        "    'learning': '#FFD23F',       # Yellow\n",
        "    'cautious': '#FF6B6B',       # Red\n",
        "    'human': '#4ECDC4',          # Teal\n",
        "    'neutral': '#E8E8E8'         # Light gray\n",
        "}\n",
        "\n",
        "class BeliefStateAgent:\n",
        "    \"\"\"AI agent with transparent belief states based on MORL principles\"\"\"\n",
        "    def __init__(self, name, learning_rate=0.1, memory_length=10):\n",
        "        self.name = name\n",
        "        self.learning_rate = learning_rate\n",
        "        self.memory_length = memory_length\n",
        "        self.trust_history = [0.5]  # Start neutral\n",
        "        \n",
        "        # Transparent belief state architecture\n",
        "        self.belief_state = {\n",
        "            'human_cooperativeness': 0.5,  # Estimated cooperation probability\n",
        "            'pattern_confidence': 0.0,      # Certainty about detected patterns\n",
        "            'risk_tolerance': 0.5,          # Willingness to trust\n",
        "            'prediction_accuracy': deque(maxlen=20)  # Track prediction performance\n",
        "        }\n",
        "        \n",
        "        self.memory = deque(maxlen=memory_length)\n",
        "        self.meta_learning = {\n",
        "            'strategy_performance': {},\n",
        "            'context_patterns': []\n",
        "        }\n",
        "        \n",
        "    def observe_human_action(self, human_action, context):\n",
        "        \"\"\"Update beliefs based on human behavior using dual inference\"\"\"\n",
        "        self.memory.append((human_action, context, time.time()))\n",
        "        \n",
        "        # Calculate cooperation rate with recency weighting\n",
        "        if len(self.memory) > 0:\n",
        "            weights = np.linspace(0.5, 1.0, len(self.memory))  # Recent actions weighted more\n",
        "            coop_actions = [1 if a == 'share' else 0 for a, _, _ in self.memory]\n",
        "            weighted_coop_rate = np.average(coop_actions, weights=weights)\n",
        "            \n",
        "            # Update beliefs with momentum\n",
        "            self.belief_state['human_cooperativeness'] = (\n",
        "                0.7 * self.belief_state['human_cooperativeness'] + \n",
        "                0.3 * weighted_coop_rate\n",
        "            )\n",
        "        \n",
        "        # Pattern detection with statistical confidence\n",
        "        if len(self.memory) >= 3:\n",
        "            recent_actions = [a for a, _, _ in self.memory[-5:]]\n",
        "            pattern_variety = len(set(recent_actions)) / len(recent_actions)\n",
        "            self.belief_state['pattern_confidence'] = 1.0 - pattern_variety\n",
        "            \n",
        "        # Update risk tolerance based on outcomes\n",
        "        if context and 'payoff_difference' in context:\n",
        "            if context['payoff_difference'] < 0:  # AI lost relative to human\n",
        "                self.belief_state['risk_tolerance'] *= 0.95\n",
        "            else:\n",
        "                self.belief_state['risk_tolerance'] = min(0.8, \n",
        "                    self.belief_state['risk_tolerance'] * 1.02)\n",
        "            \n",
        "    def decide_action(self, strategy='adaptive'):\n",
        "        \"\"\"AI decision based on belief state and strategy\"\"\"\n",
        "        # Calculate trust score from beliefs\n",
        "        trust_score = (\n",
        "            0.5 * self.belief_state['human_cooperativeness'] +\n",
        "            0.3 * self.belief_state['pattern_confidence'] +\n",
        "            0.2 * self.belief_state['risk_tolerance']\n",
        "        )\n",
        "        \n",
        "        self.trust_history.append(trust_score)\n",
        "        \n",
        "        # Strategy-specific decision making\n",
        "        if strategy == 'adaptive':\n",
        "            threshold = 0.5  # Balanced approach\n",
        "        elif strategy == 'cautious':\n",
        "            threshold = 0.7  # Require high confidence\n",
        "        elif strategy == 'generous':\n",
        "            threshold = 0.3  # Trust easily\n",
        "        else:\n",
        "            threshold = np.random.random()  # Random baseline\n",
        "            \n",
        "        # Make decision with exploration noise\n",
        "        exploration_noise = np.random.normal(0, 0.05)\n",
        "        if trust_score + exploration_noise > threshold:\n",
        "            return 'trust', trust_score\n",
        "        else:\n",
        "            return 'verify', trust_score\n",
        "            \n",
        "    def get_explanation(self):\n",
        "        \"\"\"Provide interpretable explanation of current beliefs\"\"\"\n",
        "        trust_level = self.trust_history[-1] if self.trust_history else 0.5\n",
        "        \n",
        "        if trust_level > 0.7:\n",
        "            return \"I trust you based on consistent cooperative behavior.\"\n",
        "        elif trust_level > 0.4:\n",
        "            return \"I'm learning your patterns. Building trust takes time.\"\n",
        "        else:\n",
        "            return \"I'm being cautious due to unpredictable behavior.\"\n",
        "\n",
        "class HumanAITrustSimulation:\n",
        "    \"\"\"Interactive human-AI trust dynamics with interpretability\"\"\"\n",
        "    def __init__(self):\n",
        "        self.ai_agent = BeliefStateAgent(\"AI Assistant\")\n",
        "        self.rounds = []\n",
        "        self.human_score = 0\n",
        "        self.ai_score = 0\n",
        "        \n",
        "    def play_round(self, human_choice, ai_strategy='adaptive'):\n",
        "        \"\"\"Execute one round of trust game\"\"\"\n",
        "        ai_action, trust_score = self.ai_agent.decide_action(ai_strategy)\n",
        "        \n",
        "        # Calculate outcomes based on game theory\n",
        "        if human_choice == 'share' and ai_action == 'trust':\n",
        "            human_payoff, ai_payoff = 3, 3\n",
        "            outcome = \"Mutual Benefit! ü§ù\"\n",
        "            color = AI_COLORS['confident']\n",
        "        elif human_choice == 'share' and ai_action == 'verify':\n",
        "            human_payoff, ai_payoff = 1, 2\n",
        "            outcome = \"AI was cautious üîç\"\n",
        "            color = AI_COLORS['learning']\n",
        "        elif human_choice == 'exploit' and ai_action == 'trust':\n",
        "            human_payoff, ai_payoff = 5, -1\n",
        "            outcome = \"You exploited AI's trust! üòà\"\n",
        "            color = AI_COLORS['cautious']\n",
        "        else:  # exploit and verify\n",
        "            human_payoff, ai_payoff = 0, 1\n",
        "            outcome = \"AI protected itself üõ°Ô∏è\"\n",
        "            color = AI_COLORS['neutral']\n",
        "            \n",
        "        self.human_score += human_payoff\n",
        "        self.ai_score += ai_payoff\n",
        "        \n",
        "        # AI learns from outcome\n",
        "        context = {\n",
        "            'outcome': outcome,\n",
        "            'payoff_difference': ai_payoff - human_payoff\n",
        "        }\n",
        "        self.ai_agent.observe_human_action(human_choice, context)\n",
        "        \n",
        "        round_data = {\n",
        "            'round': len(self.rounds) + 1,\n",
        "            'human_choice': human_choice,\n",
        "            'ai_action': ai_action,\n",
        "            'human_payoff': human_payoff,\n",
        "            'ai_payoff': ai_payoff,\n",
        "            'trust_score': trust_score,\n",
        "            'outcome': outcome,\n",
        "            'color': color,\n",
        "            'ai_beliefs': dict(self.ai_agent.belief_state)  # Copy current beliefs\n",
        "        }\n",
        "        \n",
        "        self.rounds.append(round_data)\n",
        "        return round_data\n",
        "    \n",
        "    def visualize_trust_evolution(self):\n",
        "        \"\"\"Create interpretable trust visualization\"\"\"\n",
        "        df = pd.DataFrame(self.rounds)\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Trust Evolution', 'Cumulative Scores', \n",
        "                          'Action History', 'AI Belief State'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
        "        )\n",
        "        \n",
        "        # Trust evolution\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=df['round'],\n",
        "                y=df['trust_score'],\n",
        "                mode='lines+markers',\n",
        "                name='AI Trust Level',\n",
        "                line=dict(color=AI_COLORS['confident'], width=3),\n",
        "                marker=dict(size=10)\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # Add trust zones\n",
        "        fig.add_hrect(y0=0.7, y1=1, \n",
        "                     fillcolor=AI_COLORS['confident'], opacity=0.2,\n",
        "                     row=1, col=1, annotation_text=\"High Trust\")\n",
        "        fig.add_hrect(y0=0.3, y1=0.7, \n",
        "                     fillcolor=AI_COLORS['learning'], opacity=0.2,\n",
        "                     row=1, col=1, annotation_text=\"Learning\")\n",
        "        fig.add_hrect(y0=0, y1=0.3, \n",
        "                     fillcolor=AI_COLORS['cautious'], opacity=0.2,\n",
        "                     row=1, col=1, annotation_text=\"Cautious\")\n",
        "        \n",
        "        # Cumulative scores\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=df['round'],\n",
        "                y=df['human_payoff'].cumsum(),\n",
        "                mode='lines',\n",
        "                name='Human Score',\n",
        "                line=dict(color=AI_COLORS['human'], width=3)\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=df['round'],\n",
        "                y=df['ai_payoff'].cumsum(),\n",
        "                mode='lines',\n",
        "                name='AI Score',\n",
        "                line=dict(color=AI_COLORS['confident'], width=3)\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # Action history\n",
        "        human_actions = df.groupby('human_choice').size()\n",
        "        ai_actions = df.groupby('ai_action').size()\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=['Share', 'Exploit'],\n",
        "                y=[human_actions.get('share', 0), human_actions.get('exploit', 0)],\n",
        "                name='Your Actions',\n",
        "                marker_color=[AI_COLORS['human'], AI_COLORS['cautious']]\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # AI belief visualization\n",
        "        if len(self.rounds) > 0:\n",
        "            latest_beliefs = self.rounds[-1]['ai_beliefs']\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=['Cooperativeness', 'Pattern\\nConfidence', 'Risk\\nTolerance'],\n",
        "                    y=[latest_beliefs['human_cooperativeness'], \n",
        "                       latest_beliefs['pattern_confidence'],\n",
        "                       latest_beliefs['risk_tolerance']],\n",
        "                    mode='markers+text',\n",
        "                    marker=dict(\n",
        "                        size=40,\n",
        "                        color=[latest_beliefs['human_cooperativeness'], \n",
        "                               latest_beliefs['pattern_confidence'],\n",
        "                               latest_beliefs['risk_tolerance']],\n",
        "                        colorscale='Viridis',\n",
        "                        showscale=True,\n",
        "                        colorbar=dict(title=\"Belief\\nStrength\")\n",
        "                    ),\n",
        "                    text=[f\"{v:.2f}\" for v in [\n",
        "                        latest_beliefs['human_cooperativeness'],\n",
        "                        latest_beliefs['pattern_confidence'],\n",
        "                        latest_beliefs['risk_tolerance']\n",
        "                    ]],\n",
        "                    textposition=\"top center\",\n",
        "                    showlegend=False\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            showlegend=True,\n",
        "            plot_bgcolor='white',\n",
        "            title_text=\"Human-AI Trust Dynamics Dashboard\"\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "\n",
        "# Create simulation instance\n",
        "sim = HumanAITrustSimulation()\n",
        "\n",
        "# Interactive interface\n",
        "output_area = widgets.Output()\n",
        "status_text = widgets.HTML(value=\"<h3>Ready to build trust with AI? ü§ñ</h3>\")\n",
        "round_counter = widgets.HTML(value=\"<b>Round: 0</b>\")\n",
        "\n",
        "share_button = widgets.Button(\n",
        "    description='Share ü§ù',\n",
        "    button_style='success',\n",
        "    tooltip='Cooperate with AI'\n",
        ")\n",
        "\n",
        "exploit_button = widgets.Button(\n",
        "    description='Exploit üòà',\n",
        "    button_style='danger',\n",
        "    tooltip='Try to exploit AI'\n",
        ")\n",
        "\n",
        "strategy_selector = widgets.RadioButtons(\n",
        "    options=['adaptive', 'cautious', 'generous'],\n",
        "    value='adaptive',\n",
        "    description='AI Strategy:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "reset_button = widgets.Button(\n",
        "    description='Reset Game',\n",
        "    button_style='warning'\n",
        ")\n",
        "\n",
        "def on_share_click(b):\n",
        "    play_round('share')\n",
        "\n",
        "def on_exploit_click(b):\n",
        "    play_round('exploit')\n",
        "    \n",
        "def on_reset_click(b):\n",
        "    global sim\n",
        "    sim = HumanAITrustSimulation()\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "    status_text.value = \"<h3>Game Reset! Ready to build trust with AI? ü§ñ</h3>\"\n",
        "    round_counter.value = \"<b>Round: 0</b>\"\n",
        "\n",
        "def play_round(choice):\n",
        "    result = sim.play_round(choice, strategy_selector.value)\n",
        "    \n",
        "    # Update status\n",
        "    status_text.value = f\"\"\"\n",
        "    <h3>{result['outcome']}</h3>\n",
        "    <p>You earned: {result['human_payoff']} | AI earned: {result['ai_payoff']}</p>\n",
        "    <p>AI Trust Level: {result['trust_score']:.2f}</p>\n",
        "    \"\"\"\n",
        "    \n",
        "    round_counter.value = f\"<b>Round: {result['round']}</b>\"\n",
        "    \n",
        "    # Update visualization\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        fig = sim.visualize_trust_evolution()\n",
        "        fig.show()\n",
        "        \n",
        "        # Show cumulative scores\n",
        "        print(f\"\\nüìä Total Scores - You: {sim.human_score} | AI: {sim.ai_score}\")\n",
        "        \n",
        "        # AI explanation\n",
        "        print(f\"\\nü§ñ AI: '{sim.ai_agent.get_explanation()}'\")\n",
        "        \n",
        "        # Interpretability insights\n",
        "        beliefs = result['ai_beliefs']\n",
        "        print(\"\\nüß† AI Belief State:\")\n",
        "        print(f\"  - Estimated cooperativeness: {beliefs['human_cooperativeness']:.2f}\")\n",
        "        print(f\"  - Pattern confidence: {beliefs['pattern_confidence']:.2f}\")\n",
        "        print(f\"  - Risk tolerance: {beliefs['risk_tolerance']:.2f}\")\n",
        "\n",
        "share_button.on_click(on_share_click)\n",
        "exploit_button.on_click(on_exploit_click)\n",
        "reset_button.on_click(on_reset_click)\n",
        "\n",
        "# Display interface\n",
        "display(status_text)\n",
        "display(round_counter)\n",
        "display(widgets.HBox([strategy_selector]))\n",
        "display(widgets.HBox([share_button, exploit_button, reset_button]))\n",
        "display(output_area)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üß™ Experiments to Try:\n",
        "#@markdown 1. **Build Maximum Trust**: Can you get the AI to trust you completely?\n",
        "#@markdown 2. **Trust Recovery**: Betray once, then try to rebuild\n",
        "#@markdown 3. **Strategy Comparison**: Which AI strategy is most robust?\n",
        "#@markdown 4. **Pattern Games**: Create patterns and see if AI learns them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Understanding AI Belief States\n",
        "\n",
        "This implementation uses transparent belief state architectures, allowing you to see exactly how the AI makes decisions:\n",
        "\n",
        "### Belief Components\n",
        "- **Human Cooperativeness**: AI's estimate of your cooperation probability\n",
        "- **Pattern Confidence**: How certain the AI is about detected patterns\n",
        "- **Risk Tolerance**: AI's willingness to trust despite uncertainty\n",
        "\n",
        "### Learning Mechanisms\n",
        "- Recency-weighted memory (recent actions matter more)\n",
        "- Dual inference: learning both behaviors and meta-patterns\n",
        "- Adaptive risk adjustment based on outcomes\n",
        "\n",
        "### Strategy Differences\n",
        "- **Adaptive**: Balances trust and caution (threshold = 0.5)\n",
        "- **Cautious**: Requires high confidence (threshold = 0.7)\n",
        "- **Generous**: Trusts more easily (threshold = 0.3)\n",
        "\n",
        "This transparency enables debugging of AI trust decisions and verification of aligned behavior."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
