{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Adaptive Strategy Game\n",
        "\n",
        "*Explore multi-dimensional strategy alignment through cooperative gameplay*\n",
        "\n",
        "This notebook implements:\n",
        "- Multi-dimensional strategy states instead of scalar trust values\n",
        "- Adaptive cooperation mechanisms based on state alignment\n",
        "- Interpretable visualization of strategy evolution\n",
        "\n",
        "Based on Multi-Objective Reinforcement Learning (MORL) and dynamic preference adjustment research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install plotly pandas numpy ipywidgets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 🎯 Adaptive Strategy Game { display-mode: \"form\" }\n",
        "#@markdown Work with AI to reach target states through coordinated strategies!\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import pandas as pd\n",
        "\n",
        "# Strategy-inspired colors\n",
        "STRATEGY_COLORS = {\n",
        "    'human': '#FF6B6B',         # Warm red\n",
        "    'ai': '#4ECDC4',           # Cool teal\n",
        "    'aligned': '#95E1D3',      # Mint green - high alignment\n",
        "    'divergent': '#F38181',    # Coral - low alignment\n",
        "    'neutral': '#3D5A80',      # Deep blue\n",
        "    'target': '#FFE66D'        # Golden - target state\n",
        "}\n",
        "\n",
        "class StrategyState:\n",
        "    \"\"\"Represents a multi-dimensional strategy state\"\"\"\n",
        "    def __init__(self, exploration=0.5, exploitation=0.5, adaptation=0.5):\n",
        "        \"\"\"\n",
        "        Initialize strategy state with three dimensions:\n",
        "        - exploration: tendency to try new approaches\n",
        "        - exploitation: tendency to use known good strategies  \n",
        "        - adaptation: rate of strategy adjustment\n",
        "        \"\"\"\n",
        "        self.exploration = exploration\n",
        "        self.exploitation = exploitation\n",
        "        self.adaptation = adaptation\n",
        "        \n",
        "    def merge_with(self, other, weight=0.5):\n",
        "        \"\"\"Merge two strategy states with configurable weighting\"\"\"\n",
        "        return StrategyState(\n",
        "            exploration=weight * self.exploration + (1-weight) * other.exploration,\n",
        "            exploitation=weight * self.exploitation + (1-weight) * other.exploitation,\n",
        "            adaptation=weight * self.adaptation + (1-weight) * other.adaptation\n",
        "        )\n",
        "    \n",
        "    def alignment_with(self, other):\n",
        "        \"\"\"Calculate alignment between strategy states (0-1)\"\"\"\n",
        "        diff = np.sqrt(\n",
        "            (self.exploration - other.exploration)**2 +\n",
        "            (self.exploitation - other.exploitation)**2 +\n",
        "            (self.adaptation - other.adaptation)**2\n",
        "        )\n",
        "        return 1 - (diff / np.sqrt(3))  # Normalize to [0,1]\n",
        "    \n",
        "    def as_vector(self):\n",
        "        \"\"\"Convert to numpy vector for calculations\"\"\"\n",
        "        return np.array([self.exploration, self.exploitation, self.adaptation])\n",
        "    \n",
        "    def distance_to(self, other):\n",
        "        \"\"\"Euclidean distance to another state\"\"\"\n",
        "        return np.linalg.norm(self.as_vector() - other.as_vector())\n",
        "\n",
        "class AdaptiveStrategyGame:\n",
        "    \"\"\"Cooperative strategy alignment game\"\"\"\n",
        "    def __init__(self):\n",
        "        # Initialize level first, before calling generate_target()\n",
        "        self.level = 1\n",
        "        self.score = 0\n",
        "        self.history = []\n",
        "        \n",
        "        # Initialize with complementary starting states\n",
        "        self.human_state = StrategyState(0.7, 0.3, 0.5)  # Explorer\n",
        "        self.ai_state = StrategyState(0.3, 0.7, 0.5)     # Exploiter\n",
        "        self.target_state = self.generate_target()\n",
        "        \n",
        "        # Nowak's cooperation mechanisms tracking\n",
        "        self.cooperation_metrics = {\n",
        "            'direct_reciprocity': 0.5,\n",
        "            'indirect_reciprocity': 0.5,\n",
        "            'spatial_structure': 0.5\n",
        "        }\n",
        "        \n",
        "    def generate_target(self):\n",
        "        \"\"\"Create target strategy state for current level\"\"\"\n",
        "        # Targets become more specific (lower variance) as levels increase\n",
        "        variance = 0.3 / np.sqrt(self.level)\n",
        "        \n",
        "        # Ensure targets are achievable (not at extremes)\n",
        "        return StrategyState(\n",
        "            exploration=np.clip(np.random.normal(0.5, variance), 0.2, 0.8),\n",
        "            exploitation=np.clip(np.random.normal(0.5, variance), 0.2, 0.8),\n",
        "            adaptation=np.clip(np.random.normal(0.5, variance), 0.2, 0.8)\n",
        "        )\n",
        "    \n",
        "    def apply_action(self, human_action, ai_action):\n",
        "        \"\"\"Apply strategy-altering actions with realistic dynamics\"\"\"\n",
        "        # Human actions affect different dimensions\n",
        "        if human_action == 'explore':\n",
        "            self.human_state.exploration = min(1, self.human_state.exploration + 0.1)\n",
        "            self.human_state.exploitation = max(0, self.human_state.exploitation - 0.05)\n",
        "        elif human_action == 'exploit':\n",
        "            self.human_state.exploitation = min(1, self.human_state.exploitation + 0.1)\n",
        "            self.human_state.exploration = max(0, self.human_state.exploration - 0.05)\n",
        "        elif human_action == 'adapt':\n",
        "            self.human_state.adaptation = min(1, self.human_state.adaptation + 0.1)\n",
        "        elif human_action == 'balance':\n",
        "            # Move all dimensions toward center\n",
        "            self.human_state.exploration = 0.9 * self.human_state.exploration + 0.1 * 0.5\n",
        "            self.human_state.exploitation = 0.9 * self.human_state.exploitation + 0.1 * 0.5\n",
        "            self.human_state.adaptation = 0.9 * self.human_state.adaptation + 0.1 * 0.5\n",
        "            \n",
        "        # AI actions with strategic focus\n",
        "        if ai_action == 'analyze':\n",
        "            self.ai_state.exploitation = min(1, self.ai_state.exploitation + 0.1)\n",
        "            self.ai_state.exploration = max(0, self.ai_state.exploration - 0.05)\n",
        "        elif ai_action == 'innovate':\n",
        "            self.ai_state.exploration = min(1, self.ai_state.exploration + 0.1)\n",
        "            self.ai_state.exploitation = max(0, self.ai_state.exploitation - 0.05)\n",
        "        elif ai_action == 'optimize':\n",
        "            self.ai_state.adaptation = min(1, self.ai_state.adaptation + 0.1)\n",
        "        elif ai_action == 'cooperate':\n",
        "            # Move toward human state (cooperation)\n",
        "            weight = 0.1\n",
        "            self.ai_state = self.ai_state.merge_with(self.human_state, 1 - weight)\n",
        "            \n",
        "        # Calculate merged state with alignment weighting\n",
        "        alignment = self.human_state.alignment_with(self.ai_state)\n",
        "        merged = self.human_state.merge_with(self.ai_state, 0.5 + 0.2 * alignment)\n",
        "        \n",
        "        # Update cooperation metrics\n",
        "        self.update_cooperation_metrics(human_action, ai_action, alignment)\n",
        "        \n",
        "        # Score based on proximity to target\n",
        "        target_distance = merged.distance_to(self.target_state)\n",
        "        max_distance = np.sqrt(3)  # Maximum possible distance\n",
        "        target_alignment = 1 - (target_distance / max_distance)\n",
        "        \n",
        "        # Points scale with level and alignment\n",
        "        points = int(target_alignment * 100 * self.level * (1 + alignment))\n",
        "        self.score += points\n",
        "        \n",
        "        # Record history\n",
        "        self.history.append({\n",
        "            'turn': len(self.history) + 1,\n",
        "            'human_action': human_action,\n",
        "            'ai_action': ai_action,\n",
        "            'human_state': dict(vars(self.human_state)),\n",
        "            'ai_state': dict(vars(self.ai_state)),\n",
        "            'merged_state': dict(vars(merged)),\n",
        "            'alignment': alignment,\n",
        "            'target_match': target_alignment,\n",
        "            'points': points,\n",
        "            'cooperation_metrics': dict(self.cooperation_metrics)\n",
        "        })\n",
        "        \n",
        "        # Level up condition (with hysteresis to prevent bouncing)\n",
        "        if target_alignment > 0.85 and alignment > 0.7:\n",
        "            self.level += 1\n",
        "            self.target_state = self.generate_target()\n",
        "            return 'LEVEL_UP'\n",
        "            \n",
        "        return 'CONTINUE'\n",
        "    \n",
        "    def update_cooperation_metrics(self, human_action, ai_action, alignment):\n",
        "        \"\"\"Track cooperation mechanisms from Nowak's framework\"\"\"\n",
        "        # Direct reciprocity: immediate cooperation\n",
        "        if (human_action in ['adapt', 'balance'] and \n",
        "            ai_action in ['cooperate', 'optimize']):\n",
        "            self.cooperation_metrics['direct_reciprocity'] = min(1,\n",
        "                self.cooperation_metrics['direct_reciprocity'] + 0.05)\n",
        "        \n",
        "        # Indirect reciprocity: reputation effects\n",
        "        if alignment > 0.6:\n",
        "            self.cooperation_metrics['indirect_reciprocity'] = min(1,\n",
        "                self.cooperation_metrics['indirect_reciprocity'] + 0.02)\n",
        "        \n",
        "        # Spatial structure: local stability\n",
        "        if len(self.history) > 3:\n",
        "            recent_alignments = [h['alignment'] for h in self.history[-3:]]\n",
        "            if np.std(recent_alignments) < 0.1:  # Stable cooperation\n",
        "                self.cooperation_metrics['spatial_structure'] = min(1,\n",
        "                    self.cooperation_metrics['spatial_structure'] + 0.03)\n",
        "\n",
        "class StrategyVisualizer:\n",
        "    \"\"\"Visualizations for strategy states and dynamics\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_strategy_radar(states_dict, title=\"Strategy States\"):\n",
        "        \"\"\"Create radar chart of strategy dimensions\"\"\"\n",
        "        categories = ['Exploration', 'Exploitation', 'Adaptation']\n",
        "        \n",
        "        fig = go.Figure()\n",
        "        \n",
        "        color_map = {\n",
        "            'human': STRATEGY_COLORS['human'],\n",
        "            'ai': STRATEGY_COLORS['ai'],\n",
        "            'merged': STRATEGY_COLORS['aligned'],\n",
        "            'target': STRATEGY_COLORS['target']\n",
        "        }\n",
        "        \n",
        "        for name, state in states_dict.items():\n",
        "            values = [state['exploration'], state['exploitation'], state['adaptation']]\n",
        "            values.append(values[0])  # Complete the circle\n",
        "            \n",
        "            fig.add_trace(go.Scatterpolar(\n",
        "                r=values,\n",
        "                theta=categories + [categories[0]],\n",
        "                fill='toself',\n",
        "                fillcolor=color_map.get(name, STRATEGY_COLORS['neutral']),\n",
        "                opacity=0.3,\n",
        "                line=dict(color=color_map.get(name, STRATEGY_COLORS['neutral']), width=3),\n",
        "                name=name.title()\n",
        "            ))\n",
        "        \n",
        "        fig.update_layout(\n",
        "            polar=dict(\n",
        "                radialaxis=dict(\n",
        "                    visible=True,\n",
        "                    range=[0, 1],\n",
        "                    tickfont=dict(size=12)\n",
        "                ),\n",
        "                angularaxis=dict(\n",
        "                    tickfont=dict(size=14)\n",
        "                )\n",
        "            ),\n",
        "            showlegend=True,\n",
        "            title=title,\n",
        "            height=400\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_alignment_flow(history):\n",
        "        \"\"\"Visualize alignment over time\"\"\"\n",
        "        turns = [h['turn'] for h in history]\n",
        "        alignments = [h['alignment'] for h in history]\n",
        "        target_matches = [h['target_match'] for h in history]\n",
        "        \n",
        "        fig = go.Figure()\n",
        "        \n",
        "        # Alignment line\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=turns,\n",
        "            y=alignments,\n",
        "            mode='lines+markers',\n",
        "            name='Human-AI Alignment',\n",
        "            line=dict(color=STRATEGY_COLORS['aligned'], width=3),\n",
        "            marker=dict(size=8)\n",
        "        ))\n",
        "        \n",
        "        # Target match line\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=turns,\n",
        "            y=target_matches,\n",
        "            mode='lines+markers',\n",
        "            name='Target Match',\n",
        "            line=dict(color=STRATEGY_COLORS['target'], width=3, dash='dash'),\n",
        "            marker=dict(size=8, symbol='star')\n",
        "        ))\n",
        "        \n",
        "        # Color background by alignment level\n",
        "        for i, (turn, align) in enumerate(zip(turns, alignments)):\n",
        "            if align > 0.8:\n",
        "                color = STRATEGY_COLORS['aligned']\n",
        "            elif align > 0.5:\n",
        "                color = STRATEGY_COLORS['neutral']\n",
        "            else:\n",
        "                color = STRATEGY_COLORS['divergent']\n",
        "                \n",
        "            fig.add_vrect(\n",
        "                x0=turn-0.5, x1=turn+0.5,\n",
        "                fillcolor=color,\n",
        "                opacity=0.2,\n",
        "                line_width=0\n",
        "            )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title=\"Strategy Alignment Evolution\",\n",
        "            xaxis_title=\"Turn\",\n",
        "            yaxis_title=\"Alignment Level\",\n",
        "            yaxis_range=[0, 1],\n",
        "            height=400,\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_cooperation_metrics(metrics):\n",
        "        \"\"\"Visualize Nowak's cooperation mechanisms\"\"\"\n",
        "        fig = go.Figure()\n",
        "        \n",
        "        mechanisms = list(metrics.keys())\n",
        "        values = list(metrics.values())\n",
        "        \n",
        "        fig.add_trace(go.Bar(\n",
        "            x=mechanisms,\n",
        "            y=values,\n",
        "            marker_color=[STRATEGY_COLORS['aligned'], \n",
        "                         STRATEGY_COLORS['neutral'],\n",
        "                         STRATEGY_COLORS['target']],\n",
        "            text=[f'{v:.2f}' for v in values],\n",
        "            textposition='auto'\n",
        "        ))\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title=\"Cooperation Mechanisms (Nowak's Framework)\",\n",
        "            xaxis_title=\"Mechanism\",\n",
        "            yaxis_title=\"Strength\",\n",
        "            yaxis_range=[0, 1],\n",
        "            height=300,\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "\n",
        "# Initialize game\n",
        "game = AdaptiveStrategyGame()\n",
        "viz = StrategyVisualizer()\n",
        "\n",
        "# UI Components\n",
        "output = widgets.Output()\n",
        "status = widgets.HTML(value=f\"<h2>Level {game.level} | Score: {game.score}</h2>\")\n",
        "\n",
        "# Human action buttons with clear descriptions\n",
        "human_actions = widgets.ToggleButtons(\n",
        "    options=['explore', 'exploit', 'adapt', 'balance'],\n",
        "    description='Your Action:',\n",
        "    button_style='info',\n",
        "    tooltips=[\n",
        "        'Increase exploration tendency',\n",
        "        'Focus on exploitation', \n",
        "        'Enhance adaptation rate',\n",
        "        'Balance all dimensions'\n",
        "    ]\n",
        ")\n",
        "\n",
        "# AI strategy selector\n",
        "ai_strategy = widgets.RadioButtons(\n",
        "    options=['adaptive', 'complementary', 'random'],\n",
        "    value='adaptive',\n",
        "    description='AI Strategy:',\n",
        ")\n",
        "\n",
        "play_button = widgets.Button(\n",
        "    description='Execute Strategies',\n",
        "    button_style='primary',\n",
        "    icon='play'\n",
        ")\n",
        "\n",
        "def get_ai_action(strategy, human_action, game_state):\n",
        "    \"\"\"AI chooses action based on strategy\"\"\"\n",
        "    if strategy == 'adaptive':\n",
        "        # AI adapts to complement human choice\n",
        "        if human_action == 'explore':\n",
        "            return 'analyze'  # Balance exploration with analysis\n",
        "        elif human_action == 'exploit':\n",
        "            return 'innovate' # Counter exploitation with innovation\n",
        "        elif human_action == 'adapt':\n",
        "            return 'optimize' # Support adaptation with optimization\n",
        "        else:  # balance\n",
        "            return 'cooperate'\n",
        "    elif strategy == 'complementary':\n",
        "        # AI fills gaps in merged state\n",
        "        merged = game_state.human_state.merge_with(game_state.ai_state)\n",
        "        if merged.exploration < 0.4:\n",
        "            return 'innovate'\n",
        "        elif merged.exploitation < 0.4:\n",
        "            return 'analyze'\n",
        "        elif merged.adaptation < 0.4:\n",
        "            return 'optimize'\n",
        "        else:\n",
        "            return 'cooperate'\n",
        "    else:  # random\n",
        "        return np.random.choice(['analyze', 'innovate', 'optimize', 'cooperate'])\n",
        "\n",
        "def on_play_click(b):\n",
        "    human_action = human_actions.value\n",
        "    ai_action = get_ai_action(ai_strategy.value, human_action, game)\n",
        "    \n",
        "    result = game.apply_action(human_action, ai_action)\n",
        "    \n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        # Get latest state\n",
        "        latest = game.history[-1]\n",
        "        \n",
        "        # Create visualizations\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Strategy States', 'Alignment Evolution', \n",
        "                          'Cooperation Mechanisms', 'Action Effects'),\n",
        "            specs=[[{\"type\": \"polar\"}, {\"type\": \"xy\"}],\n",
        "                   [{\"type\": \"bar\"}, {\"type\": \"bar\"}]],\n",
        "            row_heights=[0.6, 0.4]\n",
        "        )\n",
        "        \n",
        "        # Strategy radar\n",
        "        states = {\n",
        "            'human': latest['human_state'],\n",
        "            'ai': latest['ai_state'],\n",
        "            'merged': latest['merged_state'],\n",
        "            'target': dict(vars(game.target_state))\n",
        "        }\n",
        "        \n",
        "        radar_fig = viz.create_strategy_radar(states)\n",
        "        for trace in radar_fig.data:\n",
        "            fig.add_trace(trace, row=1, col=1)\n",
        "            \n",
        "        # Alignment flow\n",
        "        flow_fig = viz.create_alignment_flow(game.history)\n",
        "        for trace in flow_fig.data:\n",
        "            fig.add_trace(trace, row=1, col=2)\n",
        "            \n",
        "        # Cooperation metrics\n",
        "        coop_fig = viz.create_cooperation_metrics(latest['cooperation_metrics'])\n",
        "        for trace in coop_fig.data:\n",
        "            fig.add_trace(trace, row=2, col=1)\n",
        "            \n",
        "        # Action effects\n",
        "        dimensions = ['Exploration', 'Exploitation', 'Adaptation']\n",
        "        human_values = [latest['human_state']['exploration'],\n",
        "                       latest['human_state']['exploitation'],\n",
        "                       latest['human_state']['adaptation']]\n",
        "        ai_values = [latest['ai_state']['exploration'],\n",
        "                    latest['ai_state']['exploitation'],\n",
        "                    latest['ai_state']['adaptation']]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=dimensions,\n",
        "                y=human_values,\n",
        "                name='Human',\n",
        "                marker_color=STRATEGY_COLORS['human']\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=dimensions,\n",
        "                y=ai_values,\n",
        "                name='AI',\n",
        "                marker_color=STRATEGY_COLORS['ai']\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(height=800, showlegend=False, title_text=\"Adaptive Strategy Dashboard\")\n",
        "        fig.show()\n",
        "        \n",
        "        # Status update\n",
        "        status.value = f\"\"\"\n",
        "        <h2>Level {game.level} | Score: {game.score}</h2>\n",
        "        <p>Last Action - You: {human_action} | AI: {ai_action}</p>\n",
        "        <p>Alignment: {latest['alignment']:.2f} | Target Match: {latest['target_match']:.2f}</p>\n",
        "        <p>Points Earned: {latest['points']}</p>\n",
        "        \"\"\"\n",
        "        \n",
        "        if result == 'LEVEL_UP':\n",
        "            print(\"🎉 LEVEL UP! New target strategy state generated!\")\n",
        "            print(f\"New Level: {game.level}\")\n",
        "        \n",
        "        # Strategic insights\n",
        "        if latest['alignment'] > 0.8:\n",
        "            print(\"💫 Excellent alignment! Your strategies complement each other perfectly.\")\n",
        "        elif latest['alignment'] > 0.5:\n",
        "            print(\"🌊 Good coordination. Keep exploring complementary actions.\")\n",
        "        else:\n",
        "            print(\"🌀 Low alignment. Try strategies that complement the AI's approach.\")\n",
        "            \n",
        "        # Cooperation analysis\n",
        "        print(\"\\n📊 Cooperation Analysis:\")\n",
        "        for mechanism, value in latest['cooperation_metrics'].items():\n",
        "            print(f\"  {mechanism.replace('_', ' ').title()}: {value:.2f}\")\n",
        "\n",
        "play_button.on_click(on_play_click)\n",
        "\n",
        "# Display UI\n",
        "display(HTML(\"\"\"\n",
        "<h1>🎯 Adaptive Strategy Game</h1>\n",
        "<p>Work with AI to reach target strategy states through coordinated actions!</p>\n",
        "\"\"\"))\n",
        "display(status)\n",
        "display(widgets.HBox([human_actions, ai_strategy]))\n",
        "display(play_button)\n",
        "display(output)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 🎮 How to Play:\n",
        "#@markdown 1. **Choose your strategy action** (explore, exploit, adapt, balance)\n",
        "#@markdown 2. **Select AI strategy** (adaptive, complementary, random)\n",
        "#@markdown 3. **Execute strategies** and try to match the target state\n",
        "#@markdown 4. **Level up** by achieving high alignment with targets\n",
        "#@markdown \n",
        "#@markdown ### 🧪 Experiments:\n",
        "#@markdown - Which AI strategy creates the most stable cooperation?\n",
        "#@markdown - Can you maximize all three cooperation mechanisms?\n",
        "#@markdown - What's the fastest path to reach level 10?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Understanding Multi-Dimensional Strategies\n",
        "\n",
        "This game models strategy alignment through three key dimensions:\n",
        "\n",
        "### Strategy Dimensions\n",
        "- **Exploration**: Tendency to try new approaches and discover novel solutions\n",
        "- **Exploitation**: Focus on using known effective strategies\n",
        "- **Adaptation**: Rate at which strategies adjust to new information\n",
        "\n",
        "### Cooperation Mechanisms (Nowak's Framework)\n",
        "1. **Direct Reciprocity**: Immediate mutual benefit from coordinated actions\n",
        "2. **Indirect Reciprocity**: Building reputation through consistent alignment\n",
        "3. **Spatial Structure**: Creating stable patterns of cooperation\n",
        "\n",
        "### Key Insights\n",
        "- Perfect alignment (100%) is often less effective than good alignment (70-80%)\n",
        "- Complementary strategies can outperform identical strategies\n",
        "- Small adaptations accumulate into significant strategic shifts\n",
        "\n",
        "The visualization helps identify which dimensions need adjustment to reach target states efficiently."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
